В модели FCAPS сейчас основная проблема - это буковка P - performance.

Важно не просто что-то делать, а делать это с нужной скоростью! Отсюда крики про искусственный интеллект (чтобы управлять большим количеством датчиков с высокой скоростью, недоступной человеку).

В data center-е в системе хранения данных центром является фабрика коммутаторов. В одной такой фабрике может быть до 6к коммутаторов, а таких фабрик может быть несколько.
Для управления этим есть такая вещь как **SDN (Software Defined Network)**. По простому - это сеть, в которой параметры задаются кем-то централизованно.

**Поэтому буква P критична и является основной**

## Что такое производительность?

В информационной системе производительность - Response time.
В кол центре производительность - количество звонков к оператору.

Производительность - очень тяжелая характеристика, потому что есть бизнес метрики, а есть технические метрики.

Технические более-менее понятны для производительности
- `total time`
- `system time`
- `CPU time `

**Бизнес метрики определяются самой системой!**

Для разных систем это могут быть
- `response time`
- количество обращений к оператору
- количество обращений к help desk-у 

Бизнес метрики зависят от приложения, но это приложение не одно. Например, если бизнес метрика - `response time`, не понятно, `response time` чего именно?

Например, в компании может работать приложение для просмотра нормативных материалов, и может работать приложение для логистики компании. Для нас важно не просто какое-то приложение, а **критическое**.
Критическим приложением мы считаем то, которое больше денег.

>**Любые бизнес метрики – это деньги**. Если это приложение критично для нас с точки зрения денег, то это критическое приложение. Когда я еду в МСК мне навигатор нужен, а не что-то другое.

## SLA (Service-level agreement)

Допустим есть какие-то метрики, но возникает вопрос - "Как их мерять?"

> Есть рекомендация в умных книжках, что мы должны мерить в течении месяца и делать какие-то выводы, но если вы так будете делать на работе, то вас уволят, я не сомневаюсь.

Нельзя сказать за какое время нужно мерять, но нам нужны измерения при средней нагрузке, так как при пиковой могут быть разные результаты.

**Стандарта на длительность измерения нет!**

**Стандарта на то, что является бизнес-метрикой нет!**

Такой стандарт мы придумываем для себя сами и записываем его в SLA в раздел performance.
Туда мы запишем, что наша бизнес-метрика - это, например, `responce time` таких-то приложений.

## Определение бизнес-метрик и технических метрик

Мы получили какие-то метрики, например, `response time` 15 секунд или 1 минута. И нужно понять хорошо это или плохо.

> Customer должен сказать "вот это для меня комфортно, а вот это - нет"

1. **Предлагаем список бизнес-метрик**
	Мы должны предложить заказчику бизнес-метрики. Например, сказать, что есть такие бизнес-метрики:
	- `response time`
	- `quantity of calls`
	Это понятные ему бизнес-метрики
	(Такие метрики мы должны предложить по всем критическим (деньго-несущим) приложениям, которые для него основные)
	
2. **Выясняем зону комфорта заказчика**
	Далее мы спрашиваем подходят ли ему такие-то значения. Например, подходит ли для него `response time` 1 минута. Если подходит, то так и оставляем; не надо делать ее 15 секунд.
	Если его устраивает, что quantity of calls в call-center будет 2 штуки в день, то так и оставляем.
	
	. . .

Клиенту хорошо, но нас это не устраивает. Нам нужно определить для этих бизнес-метрик совокупность технических метрик. И это уже наша проблема.

	...
3. **Определяем технические метрики**
	Я буду говорить, что для выполнения таких-то бизнес-метрик есть технические метрики, например, `latency` на первом коммутаторе. И эта метрика должна равняться чему-то (указываем значение). 
	Я запишу
	- `latency` на первом коммутаторе вот такая
	- `latency` на третьем коммутаторе вот такая
	- Пропускная способность канала вот такая
	
	Так, если коммутатор описан в SLA, то его называют SLA-коммутатор. А есть коммутатор, который не описан в SLA, то есть мы про него ничего не знаем, и от него ничего не зависит. 
	
	Вот такие технические метрики мы должны создать. Их много и это большая и хитрая задача, так как тут возникает вопрос, а от чего зависят наши бизнес метрики? Запрос не простой, так как у нас много параметров в системе. Это уже вопрос нашей профессии.
	

Часть метрик будет описана SLA, а какие-то в документе **System Redness Assistant**. 
Это, скажем так, системные готовности для параметров.
И в этом документе мы говорим, что при `response time` 1 минута, у нас вот такие-то технические метрики, и их будет куча, штук 300.

## Критические отклонения метрик

> У меня была 1 минута, а стало 1.5 минуты, и чего, это плохо или хорошо?

**Если бизнес-метрики отличаются более, чем на 20%, вам не заплатят.** Это эмпирические характеристики, но они известны профессионалам.

Предпринимать какие-то действия нужно только если бизнес-метрики вышли за пределы 20% отклонения. Если отклонения в пределах 20% - не нужно ничего оптимизировать.

1. **Проверяем технические метрики**
	Если значения бизнес-метрик изменились больше чем на 20%, то я буду смотреть на технически метрики и смотреть, как изменились они. 
	Я буду мерить технические метрики. **И тут нас уже не интересует, насколько они изменились, хоть на 0,001, хоть на 10.** Любые изменения технических метрик должны настораживать.
	

Если после измерений мы смогли восстановить параметры технических метрик, можно считать, что бизне-метрика будет опять прежняя.

На деле это может быть не так, из-за множества других параметров. 

	
2. **Восстанавливаем старые технические метрики**
	Я восстанавливаю старые параметры и считаю, что `response time` станет равным значению из **System Redness Assistant**. 
	

Если этого не возникло => я неправильно определил технические метрики.

	
3. **Заново определяем технические метрики**
	Потом я их правильно определил, начал заново мерить и ...
	

> ... и тут вас уволят, потому что этот процесс продлится долго, и так делать плохо.

Поэтому надо добиться, чтобы этого не было. Это самая сложная задача, так как нет средств измерения. Непонятно, как мерить, кто будет мерить и где мерить. Если я мерею за определённым коммутатором, то я часть ошибок не отловлю, так как коммутатор их сам уберёт, а у меня будет медленно. Тут масса проблем.

Если у нас получилось, то нам свезло.  Если нет, то мы ищем те дополнительные технические метрики, которые не учли, начинаем измерения сначала, и дальше будем считать, что при их восстановлении мы снова попадём в `response time` 1 минута.

## Процедуры поиска ошибок

Подробнее было об этом на [[Метрики и модель поиска ошибок (лекция 2)#Модель поиска ошибок|прошлой лекции]].

Вот эти измерения характеристик, которые мы записали в **System Redness Assistant**, это базовые характеристики. Далее мы делаем над ними контроль и пытаемся разобраться с тем, что у нас получилось. Но работаем мы с ними, только если проблема с 20%.

Большее количество ошибок - это ошибки прикладного софта. Потом системного, потом hardware-ого. 

- Часто ошибки возникают у тех людей, которые проектируют базы данных. В базах данных есть специальные методы доступа, которые по сути - способы поиска данных на диске.
- Еще одна очень частая история - это каналы ввода/вывода. Если у нас нет систем хранения данных, и если канал ввода/вывода не заменен фабрикой коммутаторов должен быть не один.

> Ещё у нас народ талантливый, и они пытаются в интерактивном режиме заносить 10 млн записей. Вообще не понимают, что есть режим пакетный, есть режим интерактивный, и не надо 10 млн собачить в интерактивном режиме, иначе это навсегда.

В последнюю очередь надо смотреть каналы связи, если они оптоволоконные. Ошибки скорее всего в чем-то другом. **Но, могут быть проблемы с планированием систем.** Там могут быть несчастья, и там можно увеличить производительность.

> Но, сразу говорю, мы никогда не бьёмся за 2%


## Какие бывают технические метрики
### Пропускная способность канала
Теоретический максимум возможной передачи информации через канал в единицу времени.
Пропускная способность канала бывает 
- средняя
- минимальная
- максимальная
**Мы всегда меряем среднюю!** Но сейчас, пропускная способность оптоволоконных каналов такая, что о ней не особо нужно думать.

### Задержка (latency)
Latency – задержка обработки, которая бывает при работе любого оборудования, любого софта.
Она стандартизированная и регламентирована IEEE.
- Оптоволокно задерживает 1 бит на метр
- Витая пара 1-112 бит на метр задержка
- Fibre channel – специальная формула для расчёта задержки прихода фрейма FC. Там 2 Мбит на км. Там прям жёстко в документах IEEE прописано.

**Как считать задержку в Ethernet:**
Если система ethernet, то считается очень просто. 
Всегда есть сегмент, который состоит из коммутатора в центре и, например, двух станций с адаптерами. 
Задержка на 
- Первом адаптере – 100 бит/сек
- Втором адаптере – 100 бит/сек
- Коммутаторе - 100 бит/сек
Получается, мы 300 бит потеряли.

Далее мы теряем на метрах, которые ведут 
- от одного адаптера до коммутатора
- от второго адаптера до коммутатора
Это, предположим, 1-112 бит на метр.  Всё вместе не должно превышать 512 бит. 

> [!info] Latency не связана с пропускной способностью канала!
> У вас может быть, например, очень большая скорость, предположим, 2 Мбит/сек, но при этом очень длинный канал, например, 72,000 км. Даже при высоких скоростях, вы получите очень большую задержку - 0.24 сек.

### Ошибки интерфейса
Роутер, коммутатор и так далее подвержены всевозможным шумам. Об этом было в прошлой лекции про [[Метрики и модель поиска ошибок (лекция 2)#^f38ea2|ошибки ghost]]. В этом случае какие-то сетевые устройства будут некорректно работать.

При возникновении таких потерь, все испорченные пакеты будут повторно отправляться много раз. Есть такая проблема как jabber error (болтовня). Она описана у IEEE. Это когда я передаю фрейм больше, чем 1512 байт. У меня этот фрейм становится здоровым в результате шумов, неправильной работы адаптеров и так далее. В результате коммутатор будет требовать ещё раз, ещё раз, ещё раз и ещё раз. Она описана в IEEE как одна из самых опасных.

### Утилизация ресурсов
Утилизация — это характеристика, которая говорит, как мы используем в процентном соотношении ресурс в единицу времени. 
**Утилизация канала должна быть 40% при средней нагрузке и 60% при пиковой, если это ethernet.**

Утилизация – обязательная характеристика. Есть различная утилизация. 
- **Утилизация буферов ввода/вывода**
	Мы всегда используем буфера ввода/вывода, потому что любые данные считываются в буфера ввода/вывода. И есть характеристики, как мы их используем. 
	- Сколько у нас буферов использовано
	- Total - не более 80%
	- Free - не менее 20%
	Надо помнить: **80% - read 20% - write**.
	
- **Утилизация процессора**
	Утилизация любого процессора должна быть не более 80% в регулярной нагрузке (не в пике). И не важно, это процесс сервера или коммутатора. 80% не в пике, а в регулярной нагрузке. 
	Основные вопросы медленности не к процессору, а к подсистеме ввода/вывода.
	
- **утилизация любых ресурсов** 
- ... 


               Есть практические рекомендации, на что нужно обращать внимание, чтобы эти характеристики оставались нормальными.

               1) соотношение очередей к контроллеру и очередей к буферам подсистемы ввода/вывода. Пример: у нас очередь к контроллеру маленькая, а буфера ввода/вывода более 80%. Это или неправильно настроили контроллер или контроллер мёртв. А бывает наоборот. Очередь к контроллеру маленькая, а буфера загружены. Это говорит о том, что мы ввели какие-то неправильные параметры. Контроллеры не работают эффективно. И вот такие соотношения нужно смотреть всегда.

               2) Мы должны понимать, что SNMP используется везде и идея такая. Есть коммутатор, есть сервер БД, рабочая станция, роутер и ещё что-то. Есть SNMP менеджер, например, Nagios, который говорит агентам, которые расположены везде, но не на рабочей станции, не на сервере бд. Менеджер опрашивает агентов, собирает статистику, оценивает производительность. SNMP – горе, так как он понижает производительность. Делается опрос раз в 5 минут (обычно этого достаточно. Это оптимально). Если чаще, то падает производительность.

               Дальше начинается песня, чем это померить. Такой инструментарий – доп продукты софтвеерные и хардвеерные, которые позволяют произвести измерения. Очень мало продуктов меряют эти метрики. Это непросто, Российские, может, появились, я за этим следить не могу. Есть, например, netCos, которые меряет их. Он может мерить метрики и оценивать метрики, которые мы получили. Есть специальные продукты, которые меряют технические метрики. Есть, например Solar wind. Есть продукты более серьёзные, которые позволяют померить производительно в системе более сложной. Наших она не знает, мб есть, а мб нет.

               Будут специальные продукты для симуляции трафика, для генерации трафика.

               Есть реализации таких продуктов используются специальные хардвеерные средства. Есть специальные анализаторы

               SPI (Shallow Packet Inspection) То, что меряет и анализирует 1-3 OSI. В основном он смотрит в заголовок и согласно ему он определяет за какое время что-то произошло или какие данные были переданы. И всё такое прочее.

               MPI (Medium Packet Inspection) 1-5 OSI. Например, какие-то чекпоинты, но всё это старьё. Сейчас используется DPI.

               DPI (Deep packet inspection). Позволяет мерить и анализировать данные как в сессию, так и между сессиями. Позволяет анализировать ещё сами данные, а не просто заголовки. Можно смотреть что передаётся, с какой скоростью передаётся. Делается это по специальным характеристикам, которые хранятся в базах данных. Это называется сигнатура и хранится в базе данных сигнатур. По этой сигнатуре я понимаю, что передавали.

               Те, кто хочет серьёзно работать, надо крепко смотреть, потому что этот анализ будет критичным. Если файр вол зашифрован, то есть сигнатуры шифрованных трафиков, сигнатуры определённых приложений, есть сигнатуры сжатых трафиков

               Есть ещё общие рекомендации из-за чего становится медленно. Есть определённые вещи, которые часто возникают. ДНС сервера. Общее правило: никогда не подключаться к 1 ДНС серверу. Они то ли рекурсивные, то ли интерактивные, я это вам рассказывала в прошлый раз. Они могут работать вообще медленно и плохо.

               Проблемы маршрутизации. Они связаны с тем, что у нас могут возникать маршруты, которые приводят к тому, что у нас не сходимость протоколов. Вы должны одинаково видеть систему на разных роутерах.

               Периодический отказ соединения. Это приводит к потере пакетов. Это недопустимо в мультимедийных системах, но оно бывает. Это частая проблема

               Колебания маршрута – проблема ещё одна. То у нас есть маршрут от 1 роутера до другого, то он пропадает в силу несчастия с каналами связи. Если это оптоволокно, то проблем нет, если витая пара, то они могут быть и образуются черные дыры и до каких-то маршрутизаторов я не могу дойти. Это всё будет вызывать низку производительность

               Использование протоколов. Когда вы грузите ОС роутера, сервера и чего угодно, вы вечно грузите все протоколы, которые бывают в этой жизни и всегда почему-то программёры считают, что чем больше, тем лучше, а на деле чем больше, тем медленнее. Будут создаваться таблицы драйверов и ОС будет опрашивать эти таблицы. Это не 20%, но какие-то проценты вы на этом потеряете.

               Широковещательный трафик. Все приложения используют его. 10% это критично

               Низкая пропускная способность ONE каналов. Отладив локально, ты переносишь в промышленную эксплуатацию и в промышленной эксплуатации у тебя становится медленно. Обычный пользователь экономит на этих каналах ONE каналах, он экономит. Поэтому надо учитывать это при вводе в эксплуатацию.

               Бывают перегруженные интерфейсы, но я об этом не говорю

               Устаревшие интерфейсы и драйверы.

               Это всё обычные причины, почему медленно.

               В учебнике есть пример установки использования netCos. Там есть картинки с установленным netCos. Вроде, это 12 глава. С её слов. Если успеет, она расскажет.

[[Беленькая]], [[Управление и администрирование информационных систем]], [[Лекции]]